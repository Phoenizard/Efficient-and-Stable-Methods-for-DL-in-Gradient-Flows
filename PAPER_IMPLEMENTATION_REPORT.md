# Paper Implementation Report

## Reference Paper
**Title:** Efficient and stable SAV-based methods for gradient flows arising from deep learning
**Authors:** Ziqi Ma, Zhiping Mao, Jie Shen
**Journal:** Journal of Computational Physics 505 (2024) 112911
**DOI:** 10.1016/j.jcp.2024.112911

## Summary

This repository implements the SAV (Scalar Auxiliary Variable) based optimization methods described in the paper for training neural networks. The methods treat neural network training as solving gradient flows from a continuous point of view.

---

## Algorithms from the Paper

### Core SAV Methods

#### 1. **Algorithm 2: Vanilla SAV Scheme** ‚úÖ IMPLEMENTED
- **Location:** `SAV_Regression.py`, `SAV_Classification.py`
- **Key Equations:** (17a-b), (19-20)
- **Description:** Introduces auxiliary variable `r = ‚àö(I(Œ∏) + C)` to stabilize the gradient flow
- **Implementation Details:**
  ```python
  # Lines 68-80 in SAV_Regression.py
  Œ∏^(n+1,1) = Œ∏^n
  Œ∏^(n+1,2) = -Œît/‚àö(I(Œ∏^n)+C) * (I + ŒîtL)^(-1) * ‚àáI(Œ∏^n)
  r^(n+1) = r^n / (1 + Œît * (‚àáI, (I+ŒîtL)^(-1)‚àáI) / (2(I+C)))
  Œ∏^(n+1) = Œ∏^(n+1,1) + r^(n+1) * Œ∏^(n+1,2)
  ```
- **Status:** ‚úÖ Correctly matches paper formulation

#### 2. **Algorithm 3: Restart SAV Scheme** ‚úÖ NEWLY IMPLEMENTED
- **Location:** `ResSAV_Regression.py`
- **Key Feature:** Resets `rÃÇ^n = ‚àö(I(Œ∏^n) + C)` at each step
- **Purpose:** Prevents `r^n` from decaying to 0 too rapidly
- **Advantages:** Better accuracy and correct steady state solution
- **Status:** ‚úÖ Implemented according to paper specification

#### 3. **Algorithm 4: Relaxed SAV Scheme** ‚úÖ NEWLY IMPLEMENTED
- **Location:** `RelSAV_Regression.py`, `RelSAV_Classification.py`
- **Key Equations:** (22a-c), (23-24)
- **Key Feature:** Uses relaxation parameter Œæ‚ÇÄ computed from optimization
  ```python
  r^(n+1) = Œæ‚ÇÄ * rÃÉ^(n+1) + (1-Œæ‚ÇÄ) * rÃÇ^(n+1)
  Œæ‚ÇÄ = max{0, (-b - ‚àö(b¬≤-4ac)) / (2a)}
  ```
- **Purpose:** Combines vanilla SAV stability with restart SAV accuracy
- **Advantages:** Unconditionally energy stable + links r^n directly to r(t^n)
- **Status:** ‚úÖ Implemented with relaxation parameter Œ∑=0.99 (default from paper)

#### 4. **Algorithm 5: Adaptive SAV Scheme** ‚úÖ NEWLY IMPLEMENTED
- **Location:** `AdaptiveSAV_Regression.py`
- **Key Equations:** (3-4) from Adam + SAV combination
- **Key Feature:** Combines SAV with Adam's adaptive learning rate strategy
  ```python
  # Adam momentum and variance
  m^(n+1) = Œ≤‚ÇÅm^n + (1-Œ≤‚ÇÅ)‚àáI(Œ∏^n)
  v^(n+1) = Œ≤‚ÇÇv^n + (1-Œ≤‚ÇÇ)||‚àáI(Œ∏^n)||¬≤

  # Bias correction
  mÃÇ = m / (1 - Œ≤‚ÇÅ^n)
  vÃÇ = v / (1 - Œ≤‚ÇÇ^n)

  # Adaptive step size
  ŒîÃÇt = Œît / ‚àö(vÃÇ + Œµ)
  ```
- **Purpose:** Improves efficiency for complex problems
- **Parameters:** Œ≤‚ÇÅ=0.9, Œ≤‚ÇÇ=0.999, Œµ=1e-8 (standard Adam values)
- **Status:** ‚úÖ Implemented combining SAV scheme with adaptive strategy

### Space Discretization Methods

#### 5. **Algorithm 1: Smoothed Particle Method (SPM)** ‚úÖ NEWLY IMPLEMENTED
- **Location:** `SPM_SAV_Regression.py`
- **Key Equations:** (10), (12)
- **Key Feature:** Uses smooth kernel œÜ‚Çï(Œ∏ - Œ∏‚Çñ) instead of Dirac delta Œ¥(Œ∏ - Œ∏‚Çñ)
  ```python
  œÄÃÉ(Œ∏) = (1/m) Œ£‚Çñ œÜ‚Çï(Œ∏ - Œ∏‚Çñ)
  # Monte Carlo integration over Œæ ~ N(0, I)
  ```
- **Implementation:**
  - Smoothing parameter: h = 0.0001 (default from paper)
  - Monte Carlo samples: J = 10 (default from paper)
  - Uses Gaussian perturbations: Œæ ~ N(0, h¬≤I)
- **Purpose:** Better accuracy than Particle Method (PM)
- **Status:** ‚úÖ Implemented with Monte Carlo integration

---

## Comparison with Existing Code

### ‚úÖ What Was Already Correct

| File | Method | Paper Section | Status |
|------|--------|---------------|---------|
| `SAV_Regression.py` | Vanilla SAV | Algorithm 2 | ‚úÖ Correct |
| `SAV_Classification.py` | Vanilla SAV | Algorithm 2 | ‚úÖ Correct |
| `SGD_Regression.py` | SGD Baseline | Section 2 | ‚úÖ Baseline |
| `model/LinearModel.py` | One-hidden-layer NN | Section 2 | ‚úÖ Correct |

### üÜï What Was Missing (Now Implemented)

| File | Method | Paper Section | Implementation Date |
|------|--------|---------------|-------------------|
| `ResSAV_Regression.py` | Restart SAV | Algorithm 3 | 2025-12-05 |
| `RelSAV_Regression.py` | Relaxed SAV | Algorithm 4 | 2025-12-05 |
| `RelSAV_Classification.py` | Relaxed SAV (MNIST) | Algorithm 4 | 2025-12-05 |
| `AdaptiveSAV_Regression.py` | Adaptive SAV | Algorithm 5 | 2025-12-05 |
| `SPM_SAV_Regression.py` | SPM + SAV | Algorithm 1+2 | 2025-12-05 |

### ‚ö†Ô∏è What's in Code But NOT in Paper

| File | Method | Note |
|------|--------|------|
| `ESAV_Regression.py` | Exponential SAV | Uses `r = C*exp(I(Œ∏))` - different formulation |
| `ESAV_Classification.py` | Exponential SAV | Not described in the paper |
| `IEQ_Regression.py` | Implicit Euler | Uses Jacobian - not in paper |
| `IEQ_Classification.py` | Implicit Euler | Not in paper |

---

## Key Parameters from Paper

### Default Parameters (Table 1, Section 3)

| Parameter | Symbol | Default Value | Description |
|-----------|--------|---------------|-------------|
| Number of neurons | m | 100-10000 | Hidden layer width |
| SAV constant | C | 1-100 | Ensures I(Œ∏)+C ‚â• 0 |
| Linear operator coefficient | Œª | 0-10 | For L(Œ∏) = ŒªŒ∏ |
| Learning rate | Œît (lr) | 0.01-1.0 | Time step size |
| Batch size | l | 64-256 | Mini-batch size |
| Relaxation parameter | Œ∑ | 0.99 | For RelSAV method |
| SPM smoothing | h | 0.0001 | Smoothing bandwidth |
| SPM samples | J | 10 | Monte Carlo samples |
| Adam Œ≤‚ÇÅ | Œ≤‚ÇÅ | 0.9 | First moment decay |
| Adam Œ≤‚ÇÇ | Œ≤‚ÇÇ | 0.999 | Second moment decay |

---

## Numerical Examples from Paper

### Example 1 (Section 3.1.1)
**Target Function:**
```
f*(x‚ÇÅ,...,x·¥∞) = sin(Œ£p·µ¢x·µ¢) + cos(Œ£q·µ¢x·µ¢)
```
- **Dataset:** Random data in (0,1)^D
- **Dimensions tested:** D = 20, 40
- **Key finding:** SAV methods work with larger learning rates

### Example 2 (Section 3.1.2)
**Target Function:**
```
f*(x‚ÇÅ,...,x·¥∞) = Œ£ c·µ¢x·µ¢¬≤
```
- **Domain:** [0,5]^D
- **Key finding:** Adaptive SAV necessary for complex problems

### Example 3 (Section 3.1.3)
**Target Function:**
```
f*(x) = exp(-10||x||¬≤)
```
- **Dataset:** Non-uniform, x ~ N(0, 0.2)
- **Challenge:** Sharp gradients near origin
- **Key finding:** Adaptive RelSAV outperforms Adam

### Example 4 (Section 3.2)
**Dataset:** MNIST (60000 training, 10000 test)
- **Architecture:** [784,1] ‚Üí [W,a] with ReLU, m=100
- **Key finding:** SPM slightly better than PM for classification

---

## How to Reproduce Paper Results

### 1. Setup Environment
```bash
pip install torch torchvision matplotlib numpy
```

### 2. Generate Data
```bash
cd data
python data_generate.py  # For regression data
python MNIST/MNIST.py    # For MNIST data
```

### 3. Run Experiments

#### Vanilla SAV (Baseline)
```bash
python SAV_Regression.py       # Regression
python SAV_Classification.py   # MNIST classification
```

#### Restart SAV (Better Accuracy)
```bash
python ResSAV_Regression.py
```

#### Relaxed SAV (Best Balance)
```bash
python RelSAV_Regression.py
python RelSAV_Classification.py  # MNIST
```

#### Adaptive SAV (For Complex Problems)
```bash
python AdaptiveSAV_Regression.py
```

#### SPM (Higher Accuracy Space Discretization)
```bash
python SPM_SAV_Regression.py
```

### 4. Comparison with Baselines
```bash
python SGD_Regression.py        # Standard SGD
python Adam.py                  # Standard Adam (if available)
```

---

## Expected Results (From Paper)

### Stability
- **SAV methods** converge with learning rate lr=0.5-1.0
- **SGD/Adam** fail or oscillate with lr>0.1

### Efficiency
- **SAV methods** achieve 2-3 orders of magnitude better loss with same epochs
- **Adaptive SAV** converges faster than vanilla SAV for complex problems

### Accuracy
- **SPM** slightly better than **PM** (especially for classification)
- **RelSAV** better than **SAV** and **ResSAV**

### Key Findings (Figure References)
- Fig 1: Energy dissipation with full batch
- Fig 2: SPM vs PM accuracy comparison
- Fig 3: Different learning rates (lr=0.5, lr=1)
- Fig 4: Adaptive vs fixed learning rate
- Fig 10: Adaptive SAV comparison with Adam/Adagrad/RMSprop

---

## Code Structure

```
Efficient-and-Stable-Methods-for-DL-in-Gradient-Flows/
‚îú‚îÄ‚îÄ model/
‚îÇ   ‚îî‚îÄ‚îÄ LinearModel.py              # One-hidden-layer neural network
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ data_generate.py            # Generate regression data
‚îÇ   ‚îú‚îÄ‚îÄ MNIST/MNIST.py              # Load MNIST data
‚îÇ   ‚îî‚îÄ‚îÄ *.pt                        # Saved datasets
‚îú‚îÄ‚îÄ utilize.py                      # Helper functions (flatten/unflatten params)
‚îÇ
‚îú‚îÄ‚îÄ SAV_Regression.py               # ‚úÖ Algorithm 2 (Vanilla SAV)
‚îú‚îÄ‚îÄ SAV_Classification.py           # ‚úÖ Algorithm 2 (MNIST)
‚îú‚îÄ‚îÄ ResSAV_Regression.py            # üÜï Algorithm 3 (Restart SAV)
‚îú‚îÄ‚îÄ RelSAV_Regression.py            # üÜï Algorithm 4 (Relaxed SAV)
‚îú‚îÄ‚îÄ RelSAV_Classification.py        # üÜï Algorithm 4 (MNIST)
‚îú‚îÄ‚îÄ AdaptiveSAV_Regression.py       # üÜï Algorithm 5 (Adaptive SAV)
‚îú‚îÄ‚îÄ SPM_SAV_Regression.py           # üÜï Algorithm 1+2 (SPM)
‚îÇ
‚îú‚îÄ‚îÄ SGD_Regression.py               # Baseline: Standard SGD
‚îú‚îÄ‚îÄ SGD_Classification.py           # Baseline: Standard SGD (MNIST)
‚îú‚îÄ‚îÄ Adam.py                         # Baseline: Adam optimizer
‚îÇ
‚îú‚îÄ‚îÄ ESAV_Regression.py              # ‚ö†Ô∏è NOT in paper (Exponential SAV)
‚îú‚îÄ‚îÄ ESAV_Classification.py          # ‚ö†Ô∏è NOT in paper
‚îú‚îÄ‚îÄ IEQ_Regression.py               # ‚ö†Ô∏è NOT in paper (Implicit Euler)
‚îî‚îÄ‚îÄ IEQ_Classification.py           # ‚ö†Ô∏è NOT in paper
```

---

## Implementation Details

### 1. Vanilla SAV (Algorithm 2)
**Mathematical Formulation:**
```
Œ∏^(n+1) - Œ∏^n
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ + L(Œ∏^(n+1)) + (‚àáI(Œ∏^n)/‚àö(I(Œ∏^n)+C)) * r^(n+1) - L(Œ∏^n) = 0
      Œît

r^(n+1) - r^n       1
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ = ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ * (‚àáI(Œ∏^n), (Œ∏^(n+1)-Œ∏^n)/Œît)
      Œît        2‚àö(I(Œ∏^n)+C)
```

**Efficient Implementation:**
```python
Œ∏^(n+1,1) = Œ∏^n
Œ∏^(n+1,2) = -(Œît/‚àö(I(Œ∏^n)+C)) * (I + ŒîtL)^(-1) * ‚àáI(Œ∏^n)
r^(n+1) = r^n / (1 + Œît*(‚àáI, (I+ŒîtL)^(-1)‚àáI)/(2(I+C)))
Œ∏^(n+1) = Œ∏^(n+1,1) + r^(n+1) * Œ∏^(n+1,2)
```

### 2. Energy Stability
**Theorem 1:** Vanilla SAV and Relaxed SAV are unconditionally energy stable:
```
(r^(n+1))¬≤ - (r^n)¬≤ ‚â§ 0  for any Œît > 0
```

### 3. Linear Operator
```python
L(Œ∏) = Œª * (-Œî)^k * Œ∏
```
Most commonly: `L(Œ∏) = ŒªŒ∏` (k=0), where Œª ‚â• 0

---

## Performance Tuning Guide

### Choosing SAV Parameters

#### SAV Constant C
- **Small C (1-10):** For well-scaled problems
- **Large C (100-1000):** For problems with large loss values
- **Rule:** Ensure `I(Œ∏) + C > 0` always

#### Linear Operator Œª
- **Œª = 0:** Pure SAV (no damping)
- **Œª = 1-4:** Light damping (paper default)
- **Œª = 10+:** Strong damping (very smooth convergence)

#### Learning Rate Œît
- **Fixed:** 0.1 - 1.0 (much larger than SGD!)
- **Adaptive:** Start with 0.1, let Adam-style adaptation handle rest

### Method Selection Guide

| Problem Type | Recommended Method | Why |
|--------------|-------------------|-----|
| Simple regression | Vanilla SAV | Fast, simple |
| Complex regression | Adaptive SAV or RelSAV | Better convergence |
| Classification (MNIST) | RelSAV | Best balance |
| High accuracy needed | SPM + RelSAV | SPM gives better accuracy |
| Unstable training | RelSAV | Guarantees energy stability |

---

## Validation Checklist

- [x] Vanilla SAV matches Algorithm 2
- [x] Restart SAV matches Algorithm 3
- [x] Relaxed SAV matches Algorithm 4
- [x] Adaptive SAV matches Algorithm 5
- [x] SPM matches Algorithm 1
- [x] Energy dissipation verified (Fig 1)
- [ ] Reproduce Fig 2 (PM vs SPM accuracy)
- [ ] Reproduce Fig 3 (learning rate comparison)
- [ ] Reproduce Fig 4 (adaptive comparison)
- [ ] Reproduce MNIST results (Fig 11)

---

## Future Work

### To Fully Reproduce Paper:
1. ‚úÖ Implement all missing algorithms
2. ‚è≥ Run comprehensive experiments matching all paper figures
3. ‚è≥ Create benchmark comparison plots
4. ‚è≥ Add multi-layer neural network support (paper Example 5)
5. ‚è≥ Implement PDE solving examples (Burgers equation, Example 5)

### Additional Enhancements:
- [ ] Add learning rate schedulers
- [ ] Support for more activation functions (tanh, sigmoid)
- [ ] GPU optimization for large-scale problems
- [ ] Automatic parameter tuning (C, Œª)
- [ ] Visualization tools for energy evolution

---

## Citation

If you use this code, please cite the original paper:

```bibtex
@article{ma2024efficient,
  title={Efficient and stable SAV-based methods for gradient flows arising from deep learning},
  author={Ma, Ziqi and Mao, Zhiping and Shen, Jie},
  journal={Journal of Computational Physics},
  volume={505},
  pages={112911},
  year={2024},
  publisher={Elsevier},
  doi={10.1016/j.jcp.2024.112911}
}
```

---

## Contact

For questions about the implementation, please refer to:
- **Paper:** https://doi.org/10.1016/j.jcp.2024.112911
- **Code Repository:** [Current Repository]
- **Authors:** Ziqi Ma (maziqi@stu.xmu.edu.cn), Zhiping Mao (zpmao@xmu.edu.cn), Jie Shen (jshen@eitech.edu.cn)

---

*Last Updated: 2025-12-05*
*Implementation Status: ‚úÖ All core algorithms from paper implemented*
