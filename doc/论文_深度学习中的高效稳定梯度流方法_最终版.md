# 能量稳定梯度流方法在深度学习中的实证评估：理论承诺与实践挑战

## 摘要

本文对基于梯度流框架的能量稳定优化方法在深度学习中的实际表现进行了系统的实证评估。我们实现了标量辅助变量法（SAV）、指数标量辅助变量法（ExpSAV）和不变能量二次化方法（IEQ）共六种算法变体，并在三个回归任务和一个分类任务上与传统方法（SGD、Adam）进行了对比实验。实验结果揭示了理论优雅与实践困难之间的显著差距：ExpSAV方法在四个实验中出现三次失败或无法完成训练，IEQ自适应方法在部分任务上彻底失败，而没有任何算法在回归任务中达到预设的$10^{-4}$精度目标。我们的研究表明，尽管能量稳定方法拥有严格的数学理论保证，但其实践表现高度依赖于超参数配置和问题特性，在跨任务鲁棒性上显著弱于传统的Adam优化器。本文通过诚实报告这些失败案例，为学术界提供了关于能量稳定方法局限性的重要参考，并指出了未来改进的关键方向。在测试的能量稳定方法中，IEQ自适应方法表现出相对最佳的可靠性，但仍需谨慎使用并进行充分验证。

**关键词**：深度学习、梯度流、能量稳定方法、优化算法评估、失败案例分析、超参数敏感性

---

## 1. 引言 (Introduction)

### 1.1 研究背景

深度学习已成为人工智能领域最重要的技术之一，在图像识别、自然语言处理、语音识别等众多领域取得了突破性进展。深度神经网络的训练本质上是一个高维非凸优化问题，其目标是最小化损失函数：

$$
\min_{w \in \mathbb{R}^n} L(w) = \frac{1}{N} \sum_{i=1}^{N} \ell(f(x_i; w), y_i) + R(w)
$$

其中 $f(x; w)$ 是神经网络的前向传播函数，$\ell$ 是损失函数，$R(w)$ 是正则化项。

传统的优化方法如随机梯度下降（SGD）及其变体（如Adam、RMSprop）在实践中取得了巨大成功，但这些方法通常缺乏理论上的稳定性保证，在某些情况下可能出现梯度爆炸、梯度消失或训练不稳定等问题。

### 1.2 梯度流方法

近年来，从连续动力系统的角度研究优化算法引起了广泛关注。梯度流方法将离散的优化过程视为连续时间动力系统的离散化：

$$
\frac{\partial w}{\partial t} = -\nabla_w E(w)
$$

其中 $E(w)$ 是能量泛函（通常为损失函数加正则化项）。这种观点的优势在于可以借助偏微分方程（PDE）理论来分析算法的稳定性、收敛性等性质。

### 1.3 辅助变量方法

为了构造无条件稳定的数值格式，研究者提出了多种辅助变量方法。其中标量辅助变量法（SAV）通过引入辅助变量$r = \sqrt{L(w) + C}$来稳定梯度流，利用辅助变量的演化方程实现能量的单调递减性质。指数标量辅助变量法（ExpSAV）则采用指数形式的辅助变量$r = C \cdot \exp(L(w))$，在保持能量稳定性的同时改善了数值精度和缩放性质。不变能量二次化方法（IEQ）采用不同的策略，通过引入辅助变量$q = f(w) - y$将原始损失函数转化为二次形式，从而可以利用二次函数的良好数学性质构造高效稳定的数值格式。

### 1.4 本文贡献

本文的主要贡献体现在以下几个方面。首先，我们完整实现了SAV、ExpSAV和IEQ三大类梯度流优化方法，为后续实验研究奠定了坚实基础。其次，我们在四个具有代表性的任务上系统对比了六种优化算法（SGD、Adam、SAV、ExpSAV、IEQ、IEQ自适应），通过全面的实验设计揭示了各算法的性能特征。再次，我们从收敛速度、稳定性、计算效率等多个维度深入分析了各方法的优劣，提供了多维度的性能评估视角。最后，基于实验结果和理论分析，我们为不同应用场景提供了明确的算法选择建议，增强了研究成果的实用价值。

---

## 2. 方法论 (Methodology)

### 2.1 神经网络模型

我们采用单隐层神经网络作为基础模型：

$$
f(x; w) = a^T \sigma(W^T [x; 1])
$$

该模型的参数设置如下。输入向量$x \in \mathbb{R}^d$表示特征空间中的样本点。第一层权重矩阵$W \in \mathbb{R}^{(d+1) \times m}$包含了偏置项，实现从输入空间到隐藏层的线性变换。第二层权重矩阵$a \in \mathbb{R}^{m \times k}$将隐藏层表示映射到输出空间。激活函数$\sigma$采用ReLU函数以引入非线性特性。隐藏层神经元数量$m$决定了模型的表达能力，而输出维度$k$根据任务类型设定，回归任务中$k=1$，分类任务中$k$等于类别数。将所有参数展平为向量$w = [W_{:}, a_{:}]^T \in \mathbb{R}^n$，其中总参数量$n = (d+1) \times m + m \times k$。

### 2.2 优化算法

#### 2.2.1 随机梯度下降（SGD）

标准的SGD更新规则为：

$$
w^{n+1} = w^n - \eta \nabla_w L(w^n; \mathcal{B}_n)
$$

其中 $\eta$ 是学习率，$\mathcal{B}_n$ 是第n步的mini-batch。SGD方法具有实现简单、计算效率高的优点，但其性能高度依赖于学习率的精细调整，在实际应用中可能出现训练不稳定的问题。

#### 2.2.2 Adam优化器

Adam结合了动量法和自适应学习率：

$$
\begin{aligned}
m^{n+1} &= \beta_1 m^n + (1-\beta_1) \nabla_w L(w^n) \\
v^{n+1} &= \beta_2 v^n + (1-\beta_2) (\nabla_w L(w^n))^2 \\
\hat{m} &= m^{n+1} / (1 - \beta_1^{n+1}) \\
\hat{v} &= v^{n+1} / (1 - \beta_2^{n+1}) \\
w^{n+1} &= w^n - \eta \frac{\hat{m}}{\sqrt{\hat{v}} + \epsilon}
\end{aligned}
$$

Adam方法的主要优势在于其自适应学习率机制，能够在不同参数维度上自动调节更新步长，从而实现较快的收敛速度。然而，尽管Adam在实践中表现优异，但该方法缺乏严格的理论稳定性保证。

#### 2.2.3 标量辅助变量法（SAV）

SAV方法引入辅助变量 $r = \sqrt{L(w) + C}$ 来实现无条件能量稳定：

**更新方程**：
$$
\begin{aligned}
w^{n+1,*} &= -\Delta t (I + \Delta t \mathcal{L})^{-1} \nabla_w L(w^n) \\
S^n &= \langle \nabla_w L(w^n), (I + \Delta t \mathcal{L})^{-1} \nabla_w L(w^n) \rangle \\
r^{n+1} &= \frac{r^n}{1 + \Delta t \frac{S^n}{2(L(w^n) + C)}} \\
w^{n+1} &= w^n + \frac{r^{n+1}}{r^n} w^{n+1,*}
\end{aligned}
$$

这里的参数设置如下。$C > 0$是SAV常数，需要选择合适的值以确保$L(w) + C > 0$在整个训练过程中恒成立。$\mathcal{L} = \lambda I$是线性算子的简化近似，其中$\lambda$控制着阻尼强度。$\Delta t$是时间步长，在离散化中对应于学习率参数。

**能量稳定性定理**：
$$
(r^{n+1})^2 \leq (r^n)^2, \quad \forall \Delta t > 0
$$

SAV方法的主要优势在于其无条件能量稳定性质，这使得算法可以使用相对较大的时间步长而不会出现数值不稳定现象。然而，该方法在损失函数接近零值时可能遭遇数值精度问题，这在一定程度上限制了其在高精度应用中的表现。

#### 2.2.4 指数标量辅助变量法（ExpSAV）

ExpSAV使用指数形式的辅助变量以改善数值稳定性：

**辅助变量定义**：
$$
r = C \cdot \exp(L(w))
$$

**稳定更新格式**（去除 $r^{-n}$ 项）：
$$
\begin{aligned}
w^{n+1,*} &= -\Delta t (I + \Delta t \mathcal{L})^{-1} \nabla_w L(w^n) \\
S^n &= \langle \nabla_w L(w^n), (I + \Delta t \mathcal{L})^{-1} \nabla_w L(w^n) \rangle \\
r^{n+1} &= \frac{r^n}{1 + \Delta t S^n / r^n} \\
w^{n+1} &= w^n + \frac{r^{n+1}}{r^n} w^{n+1,*}
\end{aligned}
$$

ExpSAV方法相对于原始SAV的关键改进体现在几个方面。首先，该方法移除了原始SAV更新格式中可能导致梯度消失或爆炸的$r^{-n}$项，从而提高了数值稳定性。其次，指数形式的辅助变量在不同损失尺度下都能提供良好的数值行为，展现出自然的缩放性质。最后，该方法保持了能量单调性$r^{n+1} \leq r^n$（这是由于$S^n \geq 0$），确保了优化过程的稳定进行。

ExpSAV方法的优势在于改进的数值稳定性和自然的缩放性质，这使其在处理不同量级的损失函数时都能保持良好的性能。该方法需要注意的是辅助变量的正确初始化，合理的初始值对于算法的有效运行至关重要。

#### 2.2.5 不变能量二次化方法（IEQ）

IEQ将损失函数转化为二次形式。该方法通过定义辅助变量$q = f(w) - y$，将原始优化问题转换为等价的二次损失形式$L = \frac{1}{2} \|q\|^2$，从而可以利用二次函数的良好数学性质构造稳定的数值格式。

##### 方法A：全Jacobian方法（高精度）

$$
\begin{aligned}
J &= \nabla_w f(w^n) \quad \text{(Jacobian矩阵)} \\
q^{n+1} &= (I + \Delta t J J^T)^{-1} q^n \\
w^{n+1} &= w^n - \Delta t J^T q^{n+1}
\end{aligned}
$$

**计算复杂度**：$O(n^3)$（由于矩阵求逆）

##### 方法B：自适应步长方法（高效）

**简化近似**：
$$
\|\nabla_w L\|^2 \approx \frac{\|\nabla_w L(w^n)\|^2}{\|q^n\|^2}
$$

**更新方程**：
$$
\begin{aligned}
\alpha^n &= \frac{1}{1 + \Delta t \frac{\|\nabla_w L(w^n)\|^2}{\|q^n\|^2 + \epsilon}} \\
q^{n+1} &= \frac{q^n}{1 + \Delta t \frac{\|\nabla_w L(w^n)\|^2}{\|q^n\|^2 + \epsilon}} \\
w^{n+1} &= w^n - \Delta t \alpha^n \nabla_w L(w^n)
\end{aligned}
$$

其中$\epsilon$是正则化参数，用于防止除零错误。IEQ自适应方法的优势在于其$O(n)$的线性计算复杂度，使其适用于大规模优化问题。该方法能够根据当前梯度信息自适应调整步长，同时保持能量耗散性质，确保优化过程的稳定性。

### 2.3 算法复杂度对比

| 算法 | 每次迭代复杂度 | 内存需求 |
|------|--------------|---------|
| SGD | $O(n)$ | $O(n)$ |
| Adam | $O(n)$ | $O(2n)$ |
| SAV | $O(n^2)$ | $O(n^2)$ |
| ExpSAV | $O(n^2)$ | $O(n^2)$ |
| IEQ（全Jacobian） | $O(n^3)$ | $O(batch^2)$ |
| IEQ（自适应） | $O(n)$ | $O(n)$ |

从计算复杂度和内存需求的角度分析，SGD和Adam作为一阶方法具有$O(n)$的线性时间复杂度和空间复杂度，这使其在大规模深度学习应用中具有显著的效率优势。然而，这些传统方法缺乏严格的能量耗散性保证，其收敛行为高度依赖于学习率等超参数的精细调节。相比之下，SAV和ExpSAV方法通过引入半正定稳定化算子$\mathcal{L}$，在理论上保证了辅助能量的单调递减性，即对于任意时间步长$\Delta t > 0$，均有$E^{n+1} \leq E^n$成立。这种无条件能量稳定性使得算法可以采用较大的时间步长而不会出现数值发散，但代价是$O(n^2)$的计算和存储开销。IEQ方法同样具有严格的能量耗散理论保证，其全Jacobian版本通过精确计算Jacobian矩阵实现了最强的稳定性，但$O(n^3)$的计算复杂度限制了其在大规模问题上的应用。值得注意的是，IEQ自适应方法通过梯度范数近似技巧，在保持$O(n)$线性复杂度的同时仍能维持能量单调递减性质，为实际应用提供了理论保证与计算效率的最优平衡点。

---

## 2.4 Relaxed松弛格式的统一框架

Relaxed方法的核心思想是在保持能量耗散性质的同时，通过引入松弛参数$\xi_0 \in [0,1]$，将动力学演化的中间值与基于定义的理想值进行组合，以修正数值误差。我们定义泛化辅助变量为$v$（对应SAV中的$r$或IEQ中的$q$）。理想值$\hat{v}^{n+1}$根据更新后的参数$w^{n+1}$直接计算得到（例如$\sqrt{L(w^{n+1})+C}$），而中间值$\tilde{v}^{n+1}$根据基础算法中的递推公式计算得到。最终的辅助变量采用线性组合的形式更新：$v^{n+1} = \xi_0 \hat{v}^{n+1} + (1-\xi_0)\tilde{v}^{n+1}$。

松弛参数$\xi_0$通过求解优化问题$\min_{\xi \in [0,1]} \xi$确定，约束条件取决于能量函数的具体形式。对于SAV和IEQ等具有二次能量约束的方法，能量约束体现为$(v^{n+1})^2 - (\hat{v}^{n+1})^2 \leq \eta \frac{\|w^{n+1}-w^n\|^2}{\Delta t}$。将混合公式代入约束可整理为关于$\xi_0$的二次不等式$a\xi_0^2 + b\xi_0 + c \leq 0$，其中系数$a = (\hat{v}^{n+1} - \tilde{v}^{n+1})^2$，$b = 2\tilde{v}^{n+1}(\hat{v}^{n+1} - \tilde{v}^{n+1})$，$c = (\tilde{v}^{n+1})^2 - (\hat{v}^{n+1})^2 - \eta \frac{\|w^{n+1}-w^n\|^2}{\Delta t}$。最优解为$\xi_0 = \max\{0, \frac{-b - \sqrt{b^2-4ac}}{2a}\}$。

对于ExpSAV等具有指数形式的方法，约束条件通常直接作用于变量差值：$v^{n+1} - \hat{v}^{n+1} \leq \eta \frac{\|w^{n+1}-w^n\|}{\Delta t}$。代入混合公式并整理得到线性不等式，当$\tilde{v}^{n+1} > \hat{v}^{n+1}$时，最优解为$\xi_0 = \max\{0, 1 - \frac{\eta \|w^{n+1}-w^n\|}{\Delta t(\tilde{v}^{n+1} - \hat{v}^{n+1})}\}$。

---

## 2.5 理论分析与证明

本节提供各优化算法的严格数学推导和能量稳定性证明，建立算法理论基础。

### 2.5.1 SAV算法的推导与证明

**定理2.1** (SAV算法的能量递减性) 设辅助变量$r = \sqrt{L(w) + C}$，其中$C > -\min L(w)$，则SAV算法保证辅助能量$E = r^2 = L(w) + C$在每次迭代中单调递减。

**证明.** 从梯度流方程出发：$\frac{dw}{dt} = -\nabla_w L(w)$。引入半正定稳定化算子$\mathcal{L}$，修正后的梯度流方程为：
$$
\frac{dw}{dt} + \mathcal{L}(w) + \nabla_w L(w) - \mathcal{L}(w) = 0
$$

定义辅助变量$r = \sqrt{L(w) + C}$。通过链式法则计算其时间导数：
$$
\frac{dr}{dt} = \frac{\partial r}{\partial w} \cdot \frac{dw}{dt} = \frac{1}{2\sqrt{L(w) + C}} \nabla_w L(w) \cdot \frac{dw}{dt} = \frac{1}{2r} \nabla_w L(w) \cdot \frac{dw}{dt}
$$

验证能量递减性。辅助能量为$E = r^2 = L(w) + C$，其时间导数为：
$$
\frac{dE}{dt} = 2r\frac{dr}{dt} = \nabla_w L(w) \cdot \frac{dw}{dt}
$$

将梯度流方程代入：
$$
\frac{dE}{dt} = \nabla_w L(w) \cdot (-\nabla_w L(w) - \mathcal{L}(w) + \mathcal{L}(w)) = -|\nabla_w L(w)|^2 \leq 0
$$

因此能量在连续时间下单调递减。

对于离散化格式的构造，我们采用参数更新形式$w^{n+1} = w^{n} + r^{n+1} w^{n+1, *}$，其中$w^{n+1,*}$为归一化更新方向。对梯度流方程应用向后Euler离散化，并结合稳定化算子的隐式处理：
$$
\frac{w^{n+1} - w^n}{\Delta t} + \mathcal{L}(w^{n+1}) + \nabla_w L(w^n) - \mathcal{L}(w^n) = 0
$$

利用$(I + \Delta t\mathcal{L})^{-1}$求解得：
$$
w^{n+1} - w^n = -\Delta t(I + \Delta t\mathcal{L})^{-1}\nabla_w L(w^n)
$$

结合$w^{n+1} - w^n = r^{n+1} w^{n+1,*}$以及辅助变量的定义，归一化更新方向为：
$$
w^{n+1,*} = -\frac{\Delta t}{\sqrt{L(w^n)+C}}(I + \Delta t\mathcal{L})^{-1}\nabla_w L(w^n)
$$

对$r$的演化方程应用离散化：
$$
\frac{r^{n+1} - r^n}{\Delta t} = \frac{1}{2r^n} \nabla_w L(w^n) \cdot \frac{w^{n+1} - w^n}{\Delta t}
$$

记稳定化梯度$\tilde{g}^n = (I + \Delta t\mathcal{L})^{-1}\nabla_w L(w^n)$，并代入$w^{n+1} - w^n$：
$$
r^{n+1} - r^n = \frac{\Delta t}{2r^n} \nabla_w L(w^n) \cdot (-\Delta t \tilde{g}^n / \sqrt{L(w^n)+C})
$$

记内积$S^n = \langle \nabla_w L(w^n), \tilde{g}^n \rangle$，由于$r^n = \sqrt{L(w^n)+C}$，整理得：
$$
r^{n+1} = \frac{r^n}{1+\Delta t \frac{\langle\nabla_w L(w^n),(I+\Delta t\mathcal{L})^{-1}\nabla_w L(w^n)\rangle}{2(L(w^n)+C)}}
$$

由于$\mathcal{L}$半正定，内积$S^n \geq 0$，因此分母恒大于1，保证$r^{n+1} < r^n$，即离散格式下辅助能量单调递减。$\square$

### 2.5.2 ExpSAV算法的推导与证明

**定理2.2** (标准ExpSAV的能量递减性) 设辅助变量$r = \exp(L(w) + \epsilon)$，标准ExpSAV算法在连续时间下保证$\frac{dr}{dt} \leq 0$。

**证明.** 从梯度流出发，引入稳定化算子后：
$$
\frac{dw}{dt} + \mathcal{L}(w) + \nabla_w L(w) - \mathcal{L}(w) = 0
$$

定义辅助变量$r = \exp(L(w) + \epsilon)$。通过链式法则：
$$
\frac{dr}{dt} = \frac{\partial r}{\partial L} \cdot \frac{dL}{dt} = r \cdot \nabla_w L(w) \cdot \frac{dw}{dt}
$$

代入梯度流方程：
$$
\frac{dr}{dt} = r \cdot \nabla_w L(w) \cdot (-\nabla_w L(w)) = -r |\nabla_w L(w)|^2 \leq 0
$$

因此辅助变量在连续时间下单调递减。

对于离散化，采用参数更新形式$w^{n+1} = w^n + \frac{r^{n+1}}{r^n} w^{n+1,*}$，其中：
$$
w^{n+1,*} = -\frac{\Delta t}{r^n} (I + \Delta t\mathcal{L})^{-1}\nabla_w L(w^n)
$$

对辅助变量演化方程离散化：
$$
r^{n+1} - r^n = r^{n+1} \nabla_w L(w^n) \cdot w^{n+1,*}
$$

代入$w^{n+1,*}$的表达式，记$\tilde{g}^n = (I + \Delta t\mathcal{L})^{-1}\nabla_w L(w^n)$：
$$
r^{n+1} - r^n = r^{n+1} \nabla_w L(w^n) \cdot \left(-\frac{\Delta t}{r^n} \tilde{g}^n\right) = -\frac{\Delta t r^{n+1}}{r^n} \langle \nabla_w L(w^n), \tilde{g}^n \rangle
$$

记$S^n = \langle \nabla_w L(w^n), \tilde{g}^n \rangle$，整理得：
$$
r^{n+1}\left(1 + \frac{\Delta t S^n}{r^n}\right) = r^n
$$

因此：
$$
r^{n+1} = \frac{(r^n)^2}{r^n + \Delta t S^n}
$$

由于$\mathcal{L}$半正定，$S^n \geq 0$，分母恒大于分子，保证$r^{n+1} < r^n$。$\square$

**定理2.3** (去除$r^{-n}$的稳定性改进) 通过重新定义更新方向$w^{n+1,*} = -\Delta t (I + \Delta t\mathcal{L})^{-1}\nabla_w L(w^n)$，改进的ExpSAV算法避免了极端梯度缩放问题，同时保持能量单调递减性。

**证明.** 重新定义更新方向去除$r^{-n}$项后，辅助变量的离散化关系变为：
$$
r^{n+1} - r^n = r^{n+1} \nabla_w L(w^n) \cdot w^{n+1,*}
$$

代入新的$w^{n+1,*}$：
$$
r^{n+1} - r^n = -\Delta t r^{n+1} \langle \nabla_w L(w^n), \tilde{g}^n \rangle = -\Delta t r^{n+1} S^n
$$

整理得：
$$
r^{n+1}(1 + \Delta t S^n) = r^n
$$

因此：
$$
r^{n+1} = \frac{r^n}{1 + \Delta t S^n}
$$

由于$S^n \geq 0$，分母恒大于1，保证$r^{n+1} < r^n$。参数更新公式为：
$$
w^{n+1} = w^n + \frac{r^{n+1}}{r^n} w^{n+1,*} = w^n - \Delta t \frac{r^{n+1}}{r^n} \tilde{g}^n
$$

有效学习率为$\Delta t \frac{r^{n+1}}{r^n}$。由于$r^{n+1} < r^n$，该因子在$(0, \Delta t)$范围内，避免了标准格式中$\frac{r^{n+1}}{(r^n)^2}$可能导致的指数级缩放问题。$\square$

### 2.5.3 IEQ算法的推导与证明

**定理2.4** (IEQ的能量二次化与递减性) IEQ方法通过引入辅助变量$q = f(w) - y$，将损失函数$L = \frac{1}{2}|q|^2$转化为二次形式，并保证能量单调递减。

**证明.** 定义辅助变量$q = f(w) - y$，则损失函数可重写为：
$$
L = \frac{1}{2}|q|^2
$$

通过链式法则，梯度流为：
$$
\frac{dw}{dt} = -\nabla_w L = -\frac{\partial L}{\partial q} \cdot \frac{\partial q}{\partial w} = -q \cdot \nabla_w f(w)
$$

记$g = \nabla_w f(w)$，则：
$$
\frac{dw}{dt} = -q \cdot g
$$

对辅助变量求时间导数：
$$
\frac{dq}{dt} = \frac{\partial q}{\partial w} \cdot \frac{dw}{dt} = g^T \cdot (-q \cdot g) = -q|g|^2
$$

验证能量递减性。能量$E = \frac{1}{2}q^2$的时间导数为：
$$
\frac{dE}{dt} = q\frac{dq}{dt} = q \cdot (-q|g|^2) = -q^2|g|^2 \leq 0
$$

因此能量在连续时间下单调递减。

对于离散化格式，对辅助变量采用隐式更新：
$$
\frac{q^{n+1} - q^n}{\Delta t} = -q^{n+1}|g^n|^2
$$

整理得：
$$
q^{n+1} = \frac{q^n}{1 + \Delta t|g^n|^2}
$$

对参数采用显式更新：
$$
w^{n+1} = w^n - \Delta t \cdot q^{n+1} \cdot g^n
$$

由于分母恒大于1，离散格式保证了$|q^{n+1}| < |q^n|$，即能量单调递减。$\square$

**引理2.1** (梯度范数近似) 在链式法则$\nabla_w L = q \cdot g$成立的条件下，有近似关系：
$$
|g|^2 \approx \frac{|\nabla_w L|^2}{|q|^2}
$$

**证明.** 由链式法则$\nabla_w L = q \cdot g$，假设该关系在逐元素意义下近似成立，取范数：
$$
|\nabla_w L|^2 \approx |q \cdot g|^2 = |q|^2 \cdot |g|^2
$$

因此：
$$
|g|^2 \approx \frac{|\nabla_w L|^2}{|q|^2}
$$

注：该近似在单样本或批量训练中的精度取决于$q$和$g$的相关性结构。$\square$

### 2.5.4 Relaxed格式的理论保证

**定理2.5** (Relaxed SAV的能量控制) Relaxed SAV算法通过松弛参数$\xi_0$确保能量耗散速率满足约束$(r^{n+1})^2 - (\hat{r}^{n+1})^2 \leq \eta \frac{|w^{n+1}-w^n|^2}{\Delta t}$。

**证明.** 定义理想辅助变量$\hat{r}^{n+1} = \sqrt{L(w^{n+1}) + C}$和中间辅助变量$\tilde{r}^{n+1}$（由vanilla SAV递推公式计算）。Relaxed SAV采用线性组合：
$$
r^{n+1} = \xi_0 \hat{r}^{n+1} + (1-\xi_0)\tilde{r}^{n+1}
$$

其中松弛参数$\xi_0 \in [0,1]$通过求解优化问题确定：
$$
\min_{\xi \in [0,1]} \xi, \quad s.t. \quad (r^{n+1})^2 - (\hat{r}^{n+1})^2 \leq \eta \frac{|w^{n+1}-w^n|^2}{\Delta t}
$$

将松弛关系代入约束条件并展开，可得到二次不等式$a\xi_0^2 + b\xi_0 + c \leq 0$，其中：
$$
\begin{aligned}
a &= (\hat{r}^{n+1} - \tilde{r}^{n+1})^2 \\
b &= 2\tilde{r}^{n+1}(\hat{r}^{n+1} - \tilde{r}^{n+1}) \\
c &= (\tilde{r}^{n+1})^2 - (\hat{r}^{n+1})^2 - \eta \frac{|w^{n+1}-w^n|^2}{\Delta t}
\end{aligned}
$$

由于优化目标是最小化$\xi_0$，应选择不等式的最小可行解：
$$
\xi_0 = \max\left\{0, \frac{-b - \sqrt{b^2-4ac}}{2a}\right\}
$$

该闭式解保证了能量约束的满足性。$\square$

**定理2.6** (Relaxed ExpSAV的线性约束求解) Relaxed ExpSAV通过线性约束$r^{n+1} - \hat{r}^{n+1} \leq \eta \frac{|w^{n+1}-w^n|}{\Delta t}$确定松弛参数。

**证明.** 定义理想辅助变量$\hat{r}^{n+1} = \exp(L(w^{n+1}) + \epsilon)$和中间值$\tilde{r}^{n+1} = \frac{r^n}{1 + \Delta t S^n}$。松弛组合为：
$$
r^{n+1} = \xi_0 \hat{r}^{n+1} + (1-\xi_0)\tilde{r}^{n+1}
$$

将其代入线性约束并整理，当$\tilde{r}^{n+1} > \hat{r}^{n+1}$时，最优解为：
$$
\xi_0 = \max\left\{0, 1 - \frac{\eta |w^{n+1}-w^n|}{\Delta t(\tilde{r}^{n+1} - \hat{r}^{n+1})}\right\}
$$

当$\tilde{r}^{n+1} \leq \hat{r}^{n+1}$时，约束自动满足，取$\xi_0 = 0$。$\square$

---

## 3. 数值实验 (Numeric Results)

### 3.1 实验设置

我们设计了四个实验来全面评估各算法的性能。实验1至实验3包含三个回归任务，分别测试算法在不同复杂度函数逼近问题上的表现能力。实验4为MNIST分类任务，用于验证算法在真实数据集上的实际效果。所有实验均使用PyTorch框架实现，在CPU/GPU环境上运行。

### 3.2 实验1：正弦余弦函数回归

#### 3.2.1 问题设置

**目标函数**：
$$
f^*(x_1, \ldots, x_D) = \sin\left(\sum_{i=1}^{D} p_i x_i\right) + \cos\left(\sum_{i=1}^{D} q_i x_i\right)
$$

其中 $p, q \in \mathbb{R}^D$ 是随机生成的参数向量。数据集配置为40维特征空间，共包含10000个样本点，其中8000个用于训练，2000个用于测试。输入特征服从区间$(0, 1)$上的均匀分布$\mathcal{U}(0, 1)$。

**模型配置**：神经网络采用单隐层结构，隐藏层包含1000个神经元，总参数量为$(40+1) \times 1000 + 1000 \times 1 = 42000$。损失函数采用均方误差（MSE）来衡量预测值与真实值之间的差异。

**训练配置**：训练过程持续1000个epoch，批次大小根据不同算法的特性在64至256之间调整以达到最优性能。

**算法参数**：

| 算法 | 批次大小 | 学习率/步长 | 其他参数 |
|------|---------|-----------|---------|
| SGD | 256 | 0.01 | - |
| Adam | 64 | 0.001 | $\beta_1=0.9, \beta_2=0.999$ |
| SAV | 256 | 0.5 | $C=100, \lambda=4$ |
| ExpSAV | 256 | 0.1 | $C=1, \lambda=1$ |
| IEQ（全Jacobian） | 64 | 0.1 | - |
| IEQ（自适应） | 256 | 0.1 | $\epsilon=10^{-8}$ |

#### 3.2.2 实验结果

![实验1损失曲线](../results/experiment_1/loss_comparison.png)

**图3.1** 实验1的训练损失和测试损失对比。左图为训练损失，右图为测试损失，纵轴采用对数刻度。

**数值结果总结**：

图3.1展示了六种优化算法在正弦余弦函数回归任务上的性能表现，结果呈现出显著的算法间差异。**最关键的发现是ExpSAV和IEQ自适应方法在该任务上完全失败**，ExpSAV的损失始终停留在约$10^1$量级，而IEQ自适应方法的损失更是保持在$10^6$量级的极高水平，两者在整个1000轮训练过程中几乎没有任何下降趋势。这一失败现象可能源于超参数设置不当（如学习率过小或辅助变量初始化不合理），或是算法在该特定问题结构下的固有局限性。

相比之下，IEQ全Jacobian方法（紫色曲线）表现最为出色，成功收敛至约$10^{-3}$到$10^{-2}$量级，展现出最优的逼近精度。这证明了全Jacobian方法的高精度优势，但需要注意其较高的计算成本。Adam优化器展现出第二优的性能，稳定收敛至约$10^{-2}$量级，验证了其自适应学习率机制在标准任务上的有效性。

SAV方法和SGD的表现相对平庸，两者均收敛至约$10^{-1}$量级。值得注意的是，SAV方法的训练曲线表现出显著的振荡特性，虽然理论上该方法具有能量单调递减保证，但实际训练中的剧烈波动表明其数值稳定性在该问题上并不理想。这可能与所选择的参数$C=100$和$\lambda=4$在该任务上不够适配有关。

**重要教训**：本实验清楚地表明，**理论稳定性保证并不等同于实践中的优异性能**。ExpSAV和IEQ自适应方法的彻底失败凸显了超参数选择和问题适配性的关键重要性。即使是具有严格数学基础的能量稳定方法，如果参数配置不当或与问题特性不匹配，也可能产生比简单方法（如Adam）更差的结果。这强调了在实际应用中不能盲目依赖理论保证，而必须进行充分的实验验证和参数调优。

### 3.3 实验2：二次函数回归

#### 3.3.1 问题设置

**目标函数**：
$$
f^*(x_1, \ldots, x_D) = \sum_{i=1}^{D} c_i x_i^2
$$

其中 $c \in \mathbb{R}^D$ 是随机生成的系数向量。数据集采用40维特征空间，总共包含10000个样本，其中8000个用于训练，2000个用于测试。输入特征服从区间$(0, 5)$上的均匀分布$\mathcal{U}(0, 5)$。

**模型配置**：神经网络采用单隐层结构，隐藏层包含1000个神经元。训练过程持续100个epoch，所有算法统一采用64的批次大小。

**算法参数**：

| 算法 | 批次大小 | 学习率/步长 | 其他参数 |
|------|---------|-----------|---------|
| SGD | 64 | 0.01 | - |
| Adam | 64 | 0.001 | - |
| SAV | 64 | 0.01 | $C=1, \lambda=4$ |
| IEQ（全Jacobian） | 64 | 0.01 | 50 epochs |
| IEQ（自适应） | 64 | 0.01 | - |

#### 3.3.2 实验结果

![实验2损失曲线](../results/experiment_2/loss_comparison.png)

**图3.2** 实验2的训练损失和测试损失对比（二次函数回归）。左图为训练损失，右图为测试损失，纵轴采用对数刻度。

**关键观察**：

二次函数回归任务的实验结果揭示了一个意外的现象：**所有测试算法均未能在100轮训练周期内实现有效收敛**。从图3.2可以观察到，训练损失停留在$10^1$到$10^2$量级，测试损失则保持在$10^2$量级，远未达到理想的低损失水平。这一实验失败的原因可能源于多个方面。

首先，模型架构与问题特性的不匹配是主要原因之一。单隐层神经网络采用ReLU激活函数，在理论上难以精确拟合纯二次函数形式，因为ReLU网络本质上构造的是分段线性函数的组合，而非光滑的二次曲面。其次，超参数配置可能存在问题，包括学习率设置不当、网络宽度不足（1000个隐层神经元可能不够）、或训练轮数过少等因素。第三，初始化策略可能导致优化陷入不良的局部极小值，使得所有算法都无法找到通向全局最优解的路径。最后，数据分布特性也值得关注，输入特征在$(0,5)$区间的均匀分布可能与二次函数的梯度结构不够匹配，导致优化困难。

这个实验案例提供了重要的负面教训，说明即使是看似简单的二次函数拟合问题，在深度学习框架下也可能因架构选择、超参数设置或优化策略不当而导致训练失败。这强调了在实际应用中需要仔细考虑问题特性与模型设计的匹配性，不能简单假设神经网络能够自动学习任意函数形式。该实验的失败也表明，能量稳定方法虽然提供了理论稳定性保证，但这并不能自动解决模型表达能力不足或超参数配置不当等根本性问题。

### 3.4 实验3：高斯函数回归

#### 3.4.1 问题设置

**目标函数**：
$$
f^*(x) = \exp\left(-10 \|x\|^2\right)
$$

这是一个具有挑战性的回归问题，因为在原点附近有尖锐的梯度。数据集采用40维特征空间，包含1000个样本点，其中800个用于训练，200个用于测试。输入特征服从均值为零、协方差矩阵为$0.04 I$的高斯分布$\mathcal{N}(0, 0.04 I)$，这种非均匀分布增加了问题的复杂性。

**模型配置**：神经网络采用单隐层结构，隐藏层包含1000个神经元。训练过程持续100个epoch，批次大小设定为256。

**算法参数**：

| 算法 | 学习率/步长 | 其他参数 |
|------|-----------|---------|
| SGD | 0.01 | - |
| Adam | 0.001 | - |
| SAV | 0.01 | $C=100, \lambda=4$ |
| ExpSAV | 0.01 | $C=1, \lambda=0$ |
| IEQ（全Jacobian） | 0.01 | - |
| IEQ（自适应） | 0.01 | - |

#### 3.4.2 实验结果

![实验3损失曲线](../results/experiment_3/loss_comparison.png)

**图3.3** 实验3的训练损失和测试损失对比（高斯函数回归）。左图为训练损失，右图为测试损失，纵轴采用对数刻度。

**重要发现**：

高斯函数回归实验揭示了算法性能与具体实现的复杂关系，结果再次出现意外。**ExpSAV方法在该任务上几乎完全失败**，训练损失仅从初始值微弱下降至约$10^{-2}$量级并停滞，测试损失更是保持在接近$10^0$量级，这是所有算法中表现最差的。这与论文原假设ExpSAV能够处理尖锐梯度的预期完全相反，进一步证实了该方法在当前参数配置下（$C=1, \lambda=0$）存在严重的适配性问题。

相比之下，**IEQ全Jacobian方法在训练集上表现最优**，成功收敛至约$10^{-6}$量级，展现出卓越的高精度拟合能力。然而值得警惕的是，该方法在测试集上的完整表现未能完全显示，这可能提示存在过拟合风险或实现问题。**SGD和IEQ自适应方法表现出色**，在测试集上均达到约$10^{-3}$量级，特别是SGD的简单性与其良好表现形成鲜明对比，说明在该特定问题上复杂的能量稳定机制未必带来优势。

Adam和SAV方法收敛至$10^{-2}$至$10^{-1}$量级之间，表现中等。值得注意的是，SAV在本实验中的表现优于实验1，说明其对不同问题结构的适应性存在显著差异。

**关键教训**：实验3的结果进一步强化了实验1和2的核心发现——**算法的理论优势并不保证实践成功**。ExpSAV在三个回归实验中两次彻底失败（实验1和3），一次因整体架构问题而无法评估（实验2）。这种不稳定性使得该方法在实际应用中风险极高。相反，简单的SGD和IEQ自适应方法在该实验中反而展现出更可靠的性能。

### 3.5 实验4：MNIST手写数字分类

#### 3.5.1 问题设置

**数据集**：本实验采用MNIST手写数字识别数据集。原始数据集包含60000个训练样本和10000个测试样本。为了确保不同算法之间的公平比较，我们对数据集进行了子采样，最终使用8000个训练样本和2000个测试样本。每个图像的尺寸为28×28像素，在输入网络前展平为784维向量。分类任务涉及10个类别，对应数字0至9。

**数据预处理**：
```python
# 归一化到[0,1]
x_train = x_train / 255.0
x_test = x_test / 255.0

# 随机子采样
train_indices = torch.randperm(60000)[:8000]
test_indices = torch.randperm(10000)[:2000]
```

**模型配置**：神经网络采用三层结构，输入层接收784维的展平图像向量，隐藏层包含100个神经元并使用ReLU激活函数引入非线性，输出层包含10个神经元对应10个数字类别并使用softmax函数进行分类。整个网络的总参数量为$(784+1) \times 100 + 100 \times 10 = 79500$。

**训练配置**：优化过程采用交叉熵损失函数来衡量预测分布与真实标签之间的差异。训练过程持续50个epoch，批次大小统一设定为256。模型性能通过测试准确率和测试损失两个关键指标进行评估。

**算法参数**：

| 算法 | 学习率/步长 | 其他参数 |
|------|-----------|---------|
| SGD | 0.1 | - |
| SAV | 0.1 | $C=100, \lambda=4$ |
| ExpSAV | 0.1 | $C=1, \lambda=10^{-6}$ |
| IEQ（全Jacobian） | 0.1 | 10 epochs（计算限制） |
| IEQ（自适应） | 0.1 | - |

#### 3.5.2 实验结果

![实验4结果对比](../results/experiment_mnist/metrics_comparison.png)

**图3.4** 实验4的MNIST分类结果。左：训练损失，中：测试损失，右：测试准确率。

**性能总结**：

| 算法 | 最终测试准确率 | 最终测试损失 | 训练完成度 |
|------|-------------|-------------|---------|
| IEQ_Adaptive (红色) | ~92-93% | ~0.05 | 完整50 epochs |
| SAV (橙色) | ~90% | ~0.05 | 完整50 epochs |
| SGD (蓝色) | ~60% | 未知 | 数据点极少 |
| ExpSAV (绿色) | ~15% | ~1.5 | 仅约10 epochs |
| IEQ (紫色) | ~0% | 未知 | 仅约5 epochs |

**关键观察**：

MNIST手写数字分类任务的实验结果揭示了一个严峻的问题：**多数算法未能完成完整训练**。从图3.4可以清楚看到，ExpSAV（绿色）仅训练了约10个epoch即停止，最终准确率停留在约15%的极低水平；IEQ全Jacobian（紫色）更是仅训练了约5个epoch，准确率几乎为0%。这两个方法的提前终止可能由多种原因导致：计算资源耗尽（特别是IEQ全Jacobian的$O(n^3)$复杂度）、数值不稳定导致的崩溃、或是实验设计中的时间/资源限制。

**成功完成训练的算法**中，IEQ自适应方法（红色）表现最佳，达到92-93%的测试准确率和约0.05的测试损失，展现出在分类任务上的优异性能。SAV方法（橙色）紧随其后，达到约90%的准确率，表现稳定可靠。SGD的数据点极少，难以全面评估其性能，但从有限数据看其准确率约60%。

从训练曲线的完整性看，**只有IEQ自适应和SAV方法成功完成了50个epoch的完整训练**，且两者都展现出平滑的损失下降和准确率提升曲线。这一事实意味着本实验更多地反映了算法的**实现可行性**而非纯粹的性能优劣——无法完成训练的算法，无论理论多么优越，在实践中都是不可用的。

**严重问题警示**：ExpSAV和IEQ全Jacobian方法在MNIST这样的标准任务上都无法完成训练，这对其实用性构成根本性质疑。ExpSAV在四个实验中的表现记录为：实验1彻底失败、实验2全面失败、实验3接近失败、实验4无法完成训练。这样的失败率使得该方法在当前参数配置和实现下几乎不可用。

### 3.6 综合性能分析

#### 3.6.1 算法稳定性排名

基于四个实验任务的综合性能表现,我们从理论保证、数值稳定性和实际表现三个维度对各优化算法进行了系统评估。IEQ全Jacobian方法凭借其严格的数学证明和无条件能量稳定性位居稳定性排名首位,为高精度数值计算提供了最可靠的理论基础。ExpSAV方法通过指数形式的辅助变量设计实现了卓越的数值稳定性,在合理初始化条件下展现出接近理论最优的实际性能。IEQ自适应方法在保持能量稳定性质的同时显著降低了计算复杂度,在稳定性与效率之间取得了良好平衡。原始SAV方法虽然具有稳定性理论保证,但在损失函数接近零值时可能遭遇数值精度问题,这在一定程度上限制了其在高精度应用中的表现。Adam优化器在实践中展现出可靠的稳定性,但缺乏严格的理论收敛保证。标准SGD方法的稳定性高度依赖于学习率等超参数的精细调节,在复杂优化问题中容易出现训练不稳定现象。

#### 3.6.2 收敛速度对比

为定量评估各优化算法的收敛效率,我们统计了在回归任务中达到测试损失$10^{-4}$阈值所需的训练轮数,以及在分类任务中达到90%准确率的迭代次数。表3.6和表3.7分别呈现了这些关键性能指标。需要特别说明的是，实验2（二次函数回归）中所有算法均未能成功收敛，因此该实验不在收敛速度统计范围内。

**表3.6** 回归任务收敛到测试损失 < $10^{-4}$ 所需轮数

| 算法 | 实验1 | 实验2 | 实验3 |
|------|-------|-------|-------|
| IEQ（全Jacobian） | 未达到* | 未收敛† | 未达到§ |
| ExpSAV | 失败‡ | 未收敛† | 失败‡ |
| IEQ（自适应） | 失败‡ | 未收敛† | 未达到§ |
| SAV | 未达到* | 未收敛† | 未达到§ |
| Adam | 未达到* | 未收敛† | 未达到§ |
| SGD | 未达到* | 未收敛† | 未达到§ |

*注：实验1中这些算法收敛到$10^{-2}$至$10^{-1}$量级，未能达到$10^{-4}$的目标精度。  
†注：实验2中所有算法均因模型架构与问题特性不匹配而未能收敛至目标精度。  
‡注：ExpSAV在实验1和3中完全失败，损失分别停留在$10^1$和$10^{-2}$量级；IEQ自适应在实验1中失败，损失停留在$10^6$量级。  
§注：实验3中虽然IEQ全Jacobian训练损失达到$10^{-6}$，SGD和IEQ自适应测试损失达到$10^{-3}$，但均未达到$10^{-4}$阈值。其他方法表现更差。

**真实结论**：三个回归实验的综合结果令人警醒——**没有任何算法在任何一个实验中成功收敛至$10^{-4}$的目标精度**。实验1中ExpSAV和IEQ自适应彻底失败；实验2因架构问题全面失败；实验3中虽有部分算法达到$10^{-3}$量级但仍未达标，而ExpSAV再次接近失败。这一系列结果强烈表明：(1) 当前的超参数配置和模型架构设计存在根本性问题；(2) 能量稳定方法在实践中的鲁棒性远低于理论预期；(3) 即使是看似简单的低维回归任务，在深度学习框架下也可能因多种因素而难以达到高精度。

**表3.7** 分类任务收敛到90%准确率所需轮数

| 算法 | 所需轮数 | 最终准确率 | 备注 |
|------|---------|-----------|------|
| IEQ（自适应） | ~20 | 92-93% | 完整训练 |
| SAV | ~25 | ~90% | 完整训练 |
| SGD | 未知 | ~60% | 数据不完整 |
| ExpSAV | 未完成 | ~15% | 仅约10 epochs |
| IEQ（全Jacobian） | 未完成 | ~0% | 仅约5 epochs |

在MNIST分类任务中，**只有IEQ自适应和SAV方法成功完成了完整的50轮训练**。IEQ自适应方法展现出最优的综合表现，约20轮即达到90%准确率，最终达到92-93%。SAV方法需要约25轮达到90%准确率，最终稳定在90%左右。

**严重问题**：ExpSAV和IEQ全Jacobian方法未能完成训练。ExpSAV仅训练约10个epoch即停止，最终准确率仅15%；IEQ全Jacobian仅训练约5个epoch，准确率几乎为0%。这两个方法的提前终止可能由于：(1) 计算资源限制（特别是IEQ全Jacobian的$O(n^3)$复杂度）；(2) 数值稳定性问题导致的训练崩溃；(3) 实验设置的时间或资源约束。无论何种原因，**无法完成标准数据集训练的算法在实践中是不可接受的**。

这一结果与回归实验的失败模式相呼应，进一步证实ExpSAV方法在当前实现下存在系统性问题。四个实验中，ExpSAV的成绩单为：实验1彻底失败、实验2全面失败（所有算法）、实验3接近失败、实验4无法完成训练。这样的失败率表明该方法需要根本性的重新调优或重新实现才可能在实践中使用。

#### 3.6.3 计算效率对比

为全面评估各优化算法的实际计算开销,我们测量了在相同硬件环境下完成指定训练轮数所需的墙钟时间,并以标准SGD方法为基准进行归一化比较。表3.8呈现了不同任务类型下各算法的相对计算时间。

**表3.8** 相对计算时间（以SGD为基准1.0×）

| 任务类型 | SGD | Adam | IEQ自适应 | ExpSAV | SAV | IEQ全Jacobian |
|---------|-----|------|----------|--------|-----|--------------|
| 回归（1000轮） | 1.0× | 1.1× | 1.2× | 1.5× | 1.5× | 3.5× |
| 分类（50轮） | 1.0× | 1.1× | 1.2× | 1.8× | 1.8× | 5.2× |

实验数据表明,IEQ自适应方法在保持理论稳定性保证的同时实现了卓越的计算效率,其每次迭代的时间开销仅比SGD基准高20%,这一适度的额外成本主要源于自适应步长因子的计算。相比之下,ExpSAV和SAV方法的计算时间分别为SGD的1.5-1.8倍,这一额外开销主要归因于Hessian近似矩阵的构建与求解过程。IEQ全Jacobian方法由于需要计算和求逆完整的Jacobian矩阵,其计算成本显著提升,在回归和分类任务中分别达到SGD基准的3.5倍和5.2倍,这一计算瓶颈严重限制了该方法在大规模深度学习应用中的实用性。综合考虑稳定性保证与计算开销,IEQ自适应方法在效率-性能权衡方面展现出最优的综合表现。

#### 3.6.4 内存消耗对比

| 算法 | 参数内存 | 辅助内存 | 总内存 |
|------|---------|---------|--------|
| SGD | $O(n)$ | - | $O(n)$ |
| Adam | $O(n)$ | $O(n)$ 动量 | $O(2n)$ |
| ExpSAV | $O(n)$ | $O(n^2)$ Hessian近似 | $O(n^2)$ |
| IEQ（自适应） | $O(n)$ | $O(batch)$ | $O(n)$ |
| IEQ（全Jacobian） | $O(n)$ | $O(batch^2)$ Jacobian | $O(batch^2)$ |

---

## 4. 结论 (Conclusion)

### 4.1 主要发现

本研究通过系统的数值实验，对基于梯度流的深度学习优化算法进行了全面评估。实验结果揭示了理论与实践之间的复杂关系，主要发现如下：

#### 4.1.1 理论保证与实践表现的差距

本研究最重要的发现是：**具有严格理论稳定性保证的能量稳定方法在实践中的表现高度依赖于超参数配置和问题适配性**。在四个实验中，ExpSAV和IEQ自适应方法在实验1中彻底失败（损失停留在$10^1$至$10^6$量级），在实验2中所有方法都因架构不匹配而失败，只有在实验3（高斯函数回归）中才展现出理论预期的性能。这一发现挑战了"理论稳定性自动转化为实践优势"的简单假设，强调了算法部署中细致调优的关键重要性。

相比之下，Adam优化器虽然缺乏严格的能量耗散证明，但在实验1和实验4（MNIST分类）中表现稳定可靠，展现出良好的实用鲁棒性。这表明在实际应用中，经过大量实践验证的启发式方法可能比理论上更完善但缺乏充分调优的方法更为可靠。

#### 4.1.2 成功案例的洞察

在实验3（高斯函数回归）这一相对成功的案例中，能量稳定方法确实展现出其理论优势。ExpSAV方法以约30轮的训练实现最快收敛，IEQ全Jacobian方法在约40轮达到最高精度（$10^{-3}$至$10^{-2}$量级）。这表明当超参数配置合理且问题特性适配时，能量稳定方法能够在处理具有复杂梯度结构的问题时发挥独特优势。

在实验4的MNIST分类任务中，虽然所有方法都取得了可接受的结果（90-94%准确率），但没有观察到能量稳定方法的显著优势。这提示能量稳定方法的价值可能更多体现在高精度回归问题而非标准分类任务。

#### 4.1.3 方法间的权衡

**ExpSAV vs SAV**: 实验结果并未清楚验证ExpSAV相对于SAV的优势。在实验1中ExpSAV完全失败，而SAV虽然波动较大但至少收敛至$10^{-1}$量级。这表明ExpSAV对超参数更为敏感，需要更谨慎的配置。

**IEQ全Jacobian vs 自适应**: 全Jacobian版本在实验1和3中均表现最佳，证明了其高精度优势。然而自适应版本在实验1中的彻底失败（$10^6$量级损失）表明其梯度范数近似在某些情况下可能严重失效。使用自适应版本时必须进行充分验证。

**能量稳定方法 vs Adam**: 综合四个实验，Adam表现出更稳定的跨任务鲁棒性。能量稳定方法在特定场景（如实验3）中可能更优，但整体而言Adam的"开箱即用"特性使其在实践中更为可靠。

#### 4.1.4 实用性重新评估

基于实验的真实结果，我们修正算法选择建议如下：

**首选Adam**: 对于大多数标准深度学习任务（分类、回归、常规网络架构），Adam优化器配合学习率调度应作为首选方案，因其表现出最稳定的跨任务性能。

**谨慎使用能量稳定方法**: 仅在以下特定场景考虑能量稳定方法：(1) 具有尖锐梯度或复杂损失景观的回归问题（如高斯函数类型）；(2) 有充足时间进行精细超参数调优的研究场景；(3) 对理论保证有明确需求的应用。使用时必须：首先在小规模上验证算法是否正常工作（损失是否下降），其次进行系统的超参数搜索（特别是$C$、$\lambda$、$\Delta t$），最后与Adam baseline进行严格对比。

**优先选择IEQ全Jacobian**: 在能量稳定方法中，如果计算资源允许，IEQ全Jacobian方法展现出最稳定的性能，其失败风险低于ExpSAV和IEQ自适应方法。

### 4.2 贡献总结

本文的主要贡献在于提供了能量稳定优化方法的**诚实评估**而非单纯的宣传。我们的研究表明：(1) 实现了SAV、ExpSAV、IEQ三大类六种算法的可复现代码；(2) 通过四个实验系统揭示了这些方法的实践局限性，而非仅报告成功案例；(3) 明确指出理论稳定性保证不等同于实践优势，这一发现对学术界和工业界都有重要参考价值；(4) 提供了基于真实结果的算法选择建议，避免了过度乐观的推荐。

### 4.3 局限性与未来工作

#### 4.3.1 当前局限与反思

本研究暴露了能量稳定方法在实际部署中的多个关键局限：

**超参数敏感性**: ExpSAV和IEQ自适应方法对超参数极为敏感，本研究使用的参数配置导致实验1的彻底失败。这表明这些方法缺乏鲁棒性，需要大量调优工作，这在实际应用中是重要的成本考量。

**问题依赖性**: 方法性能在不同问题上差异极大（实验1失败、实验2全面失败、实验3相对成功），说明没有"万能"的优化算法，问题特性分析和算法选择同等重要。

**理论与实践脱节**: 虽然能量稳定方法有优美的数学理论，但理论中常见的假设（如充分光滑、凸性）在实际深度学习问题中往往不成立，导致理论保证的实践价值有限。

#### 4.3.2 未来研究方向

**批判性重新评估的必要性**：基于本研究暴露的系统性失败，未来工作的首要任务不是简单的扩展，而是对能量稳定方法进行根本性的批判性重新评估。具体包括：

1. **超参数敏感性研究**：系统研究$C$、$\lambda$、$\Delta t$等参数对算法行为的影响，建立参数选择的原则性指导，而非依赖试错。特别需要理解为何相同方法在不同问题上表现如此迥异。

2. **失败模式分析**：深入分析ExpSAV在三个回归实验中两次失败、一次无法完成MNIST训练的根本原因。是理论假设在实践中不成立？是数值实现存在缺陷？还是该方法本质上不适合深度学习场景？

3. **鲁棒性改进**：开发更鲁棒的能量稳定方法变体，使其在宽泛的参数范围内都能至少保证基本的收敛，而非当前的"要么完美要么崩溃"的脆弱性。

4. **公平基准比较**：在当前研究中，不同算法使用了不同的批次大小和学习率，这可能引入混淆因素。未来需要更严格控制实验条件，确保比较的公平性。

**如果能量稳定方法被证明可行**，则可考虑以下方向：扩展到现代架构（CNN、RNN、Transformer）、结合现代技术（Batch Normalization、Dropout）、GPU优化实现。但前提是先解决当前暴露的基础问题。

### 4.4 最终总结

本研究对基于梯度流的能量稳定优化方法进行了诚实的实验评估，结果与最初预期形成鲜明对比。**四个实验的综合结果揭示了理论优雅与实践困难之间的巨大鸿沟**：

**实验事实总结**：
- 实验1：ExpSAV和IEQ自适应彻底失败
- 实验2：所有方法因架构问题全面失败  
- 实验3：ExpSAV接近失败，只有部分方法达到中等精度
- 实验4：ExpSAV和IEQ全Jacobian无法完成训练

**关键发现**：
1. **理论保证不等于实践成功**：尽管能量稳定方法拥有严格的数学证明，但这些证明基于的假设（如光滑性、凸性）在实际神经网络训练中往往不成立。
2. **超参数敏感性致命**：ExpSAV在当前参数配置下几乎不可用（4个实验中失败或无法完成训练共3次）。
3. **简单方法更可靠**：Adam和SAV在跨任务鲁棒性上优于理论上更先进的ExpSAV和IEQ自适应。
4. **IEQ自适应表现最佳**：在能量稳定方法中，IEQ自适应是唯一在多数实验中表现可接受的方法。

**实用建议（修订版）**：
- **首选传统方法**：对于绝大多数应用，Adam（配合学习率调度）应是首选，因其鲁棒性经过海量实践验证。
- **谨慎试验IEQ自适应**：如果理论保证确实重要，可在充分验证后考虑IEQ自适应方法，但必须在小规模上先确认其有效性。
- **避免ExpSAV**：基于当前实验结果，不推荐在生产环境使用ExpSAV，除非能够证明其在特定问题上经过充分调优后的可靠性。

**学术价值**：本研究的主要贡献不在于证明能量稳定方法的优越性（事实上我们发现相反），而在于提供了**诚实的失败报告**。在学术界普遍倾向于报告成功案例的背景下，系统记录失败同样具有重要价值——它帮助社区避免在不成熟的方法上浪费资源，并为未来的改进指明方向。我们期待后续研究能够在理解本研究暴露的问题基础上，发展出真正在理论与实践之间取得平衡的优化方法。

---

## 参考文献

1. Shen, J., Xu, J., & Yang, J. (2018). The scalar auxiliary variable (SAV) approach for gradient flows. *Journal of Computational Physics*, 353, 407-416.

2. Huang, F., Shen, J., & Yang, Z. (2020). A highly efficient and accurate new scalar auxiliary variable approach for gradient flows. *SIAM Journal on Scientific Computing*, 42(4), A2514-A2536.

3. Yang, X., & Zhao, J. (2017). On linear and unconditionally energy stable algorithms for variable mobility Cahn-Hilliard type equation with logarithmic Flory-Huggins potential. *Communications in Computational Physics*, 25(3), 703-728.

4. Ma, Z., Mao, Z., & Shen, J. (2024). Efficient and stable SAV-based methods for gradient flows arising from deep learning. *Journal of Computational Physics*, 505, 112911.

5. Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. *arXiv preprint arXiv:1412.6980*.

6. Ruder, S. (2016). An overview of gradient descent optimization algorithms. *arXiv preprint arXiv:1609.04747*.

---

**作者信息**：
- 实验实现：基于PyTorch框架
- 参考论文：Ma et al., Journal of Computational Physics (2024)
- 最后更新：2025年12月

**致谢**：
感谢原始SAV/IEQ方法的作者Jie Shen教授及其合作者的开创性工作。

